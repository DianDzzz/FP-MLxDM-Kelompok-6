{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf159edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152691/687462750.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f34affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb51ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed_data_with_anomaly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9232396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152691/3542950029.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Lag_1_Status'] = le.fit_transform(X['Lag_1_Status'].astype(str))\n"
     ]
    }
   ],
   "source": [
    "features = ['Lag_1_Status', 'Count_Alpa_7D', 'Count_Alpa_30D', 'Streak_Alpa', 'Avg_Arrival_Time_7D', 'DayOfWeek']\n",
    "le = LabelEncoder()\n",
    "\n",
    "X = df[features]\n",
    "# Encode 'Lag_1_Status' in the features\n",
    "X['Lag_1_Status'] = le.fit_transform(X['Lag_1_Status'].astype(str))\n",
    "\n",
    "y = le.fit_transform(df['note'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c785fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Latih: 89658 baris\n",
      "Data Uji: 22415 baris\n"
     ]
    }
   ],
   "source": [
    "# 80% train-test split\n",
    "\n",
    "split_idx = int(len(df) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Data Latih: {X_train.shape[0]} baris\")\n",
    "print(f\"Data Uji: {X_test.shape[0]} baris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae70aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42, criterion='gini', max_depth=25, min_samples_leaf=30, min_samples_split=2)\n",
    "rf = RandomForestClassifier(random_state=42, max_depth=3, max_features=2, min_samples_split=2, n_estimators=80)\n",
    "xgb = XGBClassifier(eval_metric='mlogloss', random_state=42, colsample_bytree=0.7, gamma=3, learning_rate=0.2, max_depth=3, min_child_weight=0, n_estimators=180, reg_alpha=40, reg_lambda=1, seed=0)\n",
    "nb = Pipeline([\n",
    "            ('scaler', StandardScaler()), \n",
    "            ('clf', GaussianNB(var_smoothing=1e-09))\n",
    "        ])\n",
    "svm = Pipeline([\n",
    "            ('scaler', StandardScaler()), \n",
    "            ('clf', SVC(random_state=42, C=0.5, gamma='scale', kernel='rbf'))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b352ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data latih setelah resampling: 107680 baris\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE()\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(f\"Data latih setelah resampling: {X_train_res.shape[0]} baris\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d0ab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(y_test, y_pred, output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: name,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMacro F1\u001b[39m\u001b[38;5;124m'\u001b[39m: report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro avg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMacro Recall\u001b[39m\u001b[38;5;124m'\u001b[39m: report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro avg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMacro Precision\u001b[39m\u001b[38;5;124m'\u001b[39m: report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro avg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m })\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report for\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "models = [dt, rf, xgb, nb, svm]\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    y_pred = model.fit(X_train_res, y_train_res).predict(X_test)\n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Macro F1': report['macro avg']['f1-score'],\n",
    "        'Macro Recall': report['macro avg']['recall'],\n",
    "        'Macro Precision': report['macro avg']['precision']\n",
    "    })\n",
    "    print(\"Classification Report for\", model.__class__.__name__)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for all metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Macro F1', 'Macro Recall', 'Macro Precision']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'lightsalmon']\n",
    "\n",
    "for idx, (ax, metric, color) in enumerate(zip(axes.flat, metrics, colors)):\n",
    "  ax.barh(results_df['Model'], results_df[metric], color=color, edgecolor='navy')\n",
    "  ax.set_xlabel(metric, fontsize=11)\n",
    "  ax.set_ylabel('Model', fontsize=11)\n",
    "  ax.set_title(f'Comparison of {metric} Across Models', fontsize=12)\n",
    "  ax.set_xlim(0, max(results_df[metric]) * 1.15)\n",
    "  \n",
    "  # Add value labels on bars\n",
    "  for i, (model, score) in enumerate(zip(results_df['Model'], results_df[metric])):\n",
    "    ax.text(score + 0.005, i, f'{score:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
