{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b544a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a978aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_df = pd.read_csv(\"attendance_with_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884eea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = featured_df.copy()\n",
    "df_model['date'] = pd.to_datetime(df_model['date'], errors='coerce')\n",
    "\n",
    "features = ['Count_Telat_7D', 'Count_Alpa_30D', 'Streak_Telat', 'Avg_Arrival_Time_7D']\n",
    "\n",
    "df_model['has_checkin'] = df_model['checkin_time'].notna().astype(int)\n",
    "df_model['Lag_1_Status_filled'] = df_model['Lag_1_Status'].fillna('None')\n",
    "cat_cols = ['DayOfWeek', 'Lag_1_Status_filled']\n",
    "\n",
    "X_num = df_model[features + ['has_checkin']].copy()\n",
    "X_num['Avg_Arrival_Time_7D'] = X_num['Avg_Arrival_Time_7D'].fillna(X_num['Avg_Arrival_Time_7D'].median())\n",
    "\n",
    "X_cat = pd.get_dummies(df_model[cat_cols].astype(str), prefix=cat_cols, drop_first=True)\n",
    "X = pd.concat([X_num, X_cat], axis=1)\n",
    "y = df_model['note'].astype(str) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8298667",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train_enc)\n",
    "\n",
    "y_pred_enc = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification metrics (encoded labels):\n",
      "Accuracy: 0.9116\n",
      "Precision (macro / weighted): 0.7946 / 0.9134\n",
      "Recall    (macro / weighted): 0.7968 / 0.9116\n",
      "F1-score  (macro / weighted): 0.7956 / 0.9125\n",
      "\n",
      "Classification report (string labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        alpa       0.99      0.99      0.99      9246\n",
      "       hadir       0.89      0.88      0.89     11407\n",
      "       libur       0.99      0.99      0.99      9402\n",
      "       telat       0.31      0.33      0.32      1852\n",
      "\n",
      "    accuracy                           0.91     31907\n",
      "   macro avg       0.79      0.80      0.80     31907\n",
      "weighted avg       0.91      0.91      0.91     31907\n",
      "\n",
      "Confusion matrix (string labels):\n",
      "[[ 9142     0   104     0]\n",
      " [    0 10069     0  1338]\n",
      " [  138     0  9264     0]\n",
      " [    0  1240     0   612]]\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test_enc, y_pred_enc)\n",
    "prec_macro = precision_score(y_test_enc, y_pred_enc, average='macro', zero_division=0)\n",
    "rec_macro = recall_score(y_test_enc, y_pred_enc, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(y_test_enc, y_pred_enc, average='macro', zero_division=0)\n",
    "prec_weight = precision_score(y_test_enc, y_pred_enc, average='weighted', zero_division=0)\n",
    "rec_weight = recall_score(y_test_enc, y_pred_enc, average='weighted', zero_division=0)\n",
    "f1_weight = f1_score(y_test_enc, y_pred_enc, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"Classification metrics (encoded labels):\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision (macro / weighted): {prec_macro:.4f} / {prec_weight:.4f}\")\n",
    "print(f\"Recall    (macro / weighted): {rec_macro:.4f} / {rec_weight:.4f}\")\n",
    "print(f\"F1-score  (macro / weighted): {f1_macro:.4f} / {f1_weight:.4f}\")\n",
    "\n",
    "# human-readable report using original label strings\n",
    "y_pred_labels = le.inverse_transform(y_pred_enc)\n",
    "print(\"\\nClassification report (string labels):\")\n",
    "print(classification_report(y_test, y_pred_labels, zero_division=0))\n",
    "\n",
    "print(\"Confusion matrix (string labels):\")\n",
    "print(confusion_matrix(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ead72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# joblib.dump({\"model\": dt_reg, \"label_encoder\": le}, \"dt_regressor_with_le.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
